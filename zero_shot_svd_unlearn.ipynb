{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g84Cut_MhBv"
      },
      "source": [
        "# SVD Unlearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrfVnhoMMUp0"
      },
      "source": [
        "## Clone class_forgetting repo and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3eGE2ejLlkJ",
        "outputId": "06ec9baf-1696-4e08-c5e6-c872e457b09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'class_forgetting'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 136 (delta 56), reused 69 (delta 25), pack-reused 17 (from 1)\u001b[K\n",
            "Receiving objects: 100% (136/136), 69.84 MiB | 30.37 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GittyHarsha/class_forgetting.git\n",
        "\n",
        "%mv /content/class_forgetting/utils.py /content\n",
        "%mv /content/class_forgetting/pretrained_models /content/pretrained_models\n",
        "%mv /content/class_forgetting/models /content/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xGysFVHLvyp",
        "outputId": "9ccda60e-bfe5-48a7-f7b7-ffe2c973f03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scienceplots\n",
            "  Downloading SciencePlots-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from scienceplots) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scienceplots) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->scienceplots) (1.17.0)\n",
            "Downloading SciencePlots-2.1.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: scienceplots\n",
            "Successfully installed scienceplots-2.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install scienceplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_INjvIFlLwM3"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import os\n",
        "from utils import get_dataset, get_model, test, SVC_MIA\n",
        "import copy\n",
        "import os\n",
        "import random\n",
        "from torch import nn\n",
        "from collections import OrderedDict, Counter\n",
        "\n",
        "from multiprocessing import Pool\n",
        "from multiprocessing import Process, Value, Array\n",
        "from functools import partial\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import scienceplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLR3f3jPMccK"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kyhn7cLGLyNS"
      },
      "outputs": [],
      "source": [
        "def get_projection_matrix(device, Mr, Mf):\n",
        "    update_dict = OrderedDict()\n",
        "    for act in Mr.keys():\n",
        "        mr = Mr[act]\n",
        "        mf = Mf[act]\n",
        "        I = torch.eye(mf.shape[0]).to(device)\n",
        "        update_dict[act] =  I  - (mf - torch.mm(mf,mr) )\n",
        "    return update_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otySY2q_MmT_"
      },
      "source": [
        "## Unlearn method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B6KdOeXTL1xh"
      },
      "outputs": [],
      "source": [
        "def svd_unlearn(args, model, device, retain_loader, forget_loader, train_loader, test_loader, train_dataset, val_index = None, custom=False, **kwargs):\n",
        "\n",
        "    model.eval() # Ensures batch statistics do not change.\n",
        "    # get 100 images of each class other than unlearning class\n",
        "    index_list = []\n",
        "    targets = np.array(train_dataset.targets)\n",
        "    for i in range(args.num_classes):\n",
        "        if i !=  args.unlearn_class[0]:\n",
        "            class_i_index = np.intersect1d(np.where(i == targets)[0], val_index)\n",
        "            index_list.extend(class_i_index[:int(args.our_samples//(args.num_classes-1))])\n",
        "    if custom:\n",
        "        small_retain_loader = retain_loader\n",
        "        small_forget_loader = forget_loader\n",
        "    else:\n",
        "      small_retain_loader = torch.utils.data.DataLoader( torch.utils.data.Subset(train_dataset, index_list), batch_size=args.our_samples , shuffle=True)\n",
        "      small_forget_loader = torch.utils.data.DataLoader( forget_loader.dataset , batch_size=args.our_samples , shuffle=True)\n",
        "    print(len(small_retain_loader.dataset))\n",
        "    print(len(small_forget_loader.dataset))\n",
        "    with torch.no_grad():\n",
        "        for data, target in small_retain_loader:\n",
        "        # for data, target in retain_loader\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # Rr = model.get_activations(data)\n",
        "            Mr = model.get_scaled_projections(data, args.our_alpha_r, args.our_max_patches)\n",
        "            break\n",
        "        # print(Counter(target.tolist()))\n",
        "\n",
        "        for data, target in small_forget_loader:\n",
        "        # for data, target in forget_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # Rf = model.get_activations(data)\n",
        "            Mf = model.get_scaled_projections(data, args.our_alpha_f, args.our_max_patches)\n",
        "            break\n",
        "        # print(Counter(target.tolist()))\n",
        "\n",
        "    # model.project_weights(get_projection_matrix(device=device, Rr=Rr, Rf=Rf, alpha_r = args.our_alpha_r, alpha_f = args.our_alpha_f, update_dict=OrderedDict()) )\n",
        "    model.project_weights(get_projection_matrix(device, Mr, Mf))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ8JV8owMppr"
      },
      "source": [
        "## train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YZkVqyy9L5QU"
      },
      "outputs": [],
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch, mode = \"descent\", clip=None):\n",
        "    model.train()\n",
        "    train_loss= 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        if mode == \"ascent\":\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad.data *= -1.0\n",
        "            if clip is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.detach().item()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()), end='\\r')\n",
        "           # wandb.log({\"train_loss\":loss.item() })\n",
        "            if args.dry_run:\n",
        "                break\n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEEHXCkQMrTV"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QKDe6kIgL7B0"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='PyTorch cifar10 Example')\n",
        "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                    help='input batch size for training (default: 64)')\n",
        "parser.add_argument('--dataset', type=str, default=\"gtsrb\",\n",
        "                    help='')\n",
        "parser.add_argument('--test-batch-size', type=int, default=512, metavar='N',\n",
        "                    help='input batch size for testing (default: 1000)')\n",
        "parser.add_argument('--epochs-or-steps', type=int, default=20, metavar='N',\n",
        "                    help='number of epochs to train (default: 100)')\n",
        "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
        "                    help='')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, metavar='LR',\n",
        "                    help='')\n",
        "parser.add_argument('--weight-decay', type=float, default=5e-4, metavar='LR',\n",
        "                    help='')\n",
        "parser.add_argument('--gamma', type=float, default=0.5, metavar='M',\n",
        "                    help='Learning rate step gamma (default: 0.7) after 50 epochs')\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='disables CUDA training')\n",
        "parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                    help='disables macOS GPU training')\n",
        "parser.add_argument('--dry-run', action='store_true', default=False,\n",
        "                    help='quickly check a single pass')\n",
        "parser.add_argument('--seed', type=int, default=1234, metavar='S',\n",
        "                    help='random seed (default: 1)')\n",
        "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
        "                    help='how many batches to wait before logging training status')\n",
        "parser.add_argument('--no-train-transform', action='store_true', default=False,\n",
        "                    help='For Saving the current Model')\n",
        "parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                    help='For Saving the current Model')\n",
        "parser.add_argument('--arch', type=str, default='resnet34',\n",
        "                    help='')\n",
        "parser.add_argument('--data-path', type=str, default='../data/',\n",
        "                    help='')\n",
        "parser.add_argument('--val-set-mode', action='store_true', default=False,\n",
        "                    help='For Saving the current Model')\n",
        "parser.add_argument('--val-set-samples', type=int, default=10000, metavar='EPS',\n",
        "                    help='')\n",
        "### Unlearn parameters\n",
        "\n",
        "parser.add_argument('--num-retain-samples', type=int, default=45000,\n",
        "                    help='')\n",
        "parser.add_argument('--num-forget-samples', type=int, default=5000,\n",
        "                    help='')\n",
        "parser.add_argument('--grad-norm-clip', type=float, default=None,\n",
        "                    help='')\n",
        "parser.add_argument('--unlearn-class', type=str, default=\"9\",\n",
        "                    help='')\n",
        "parser.add_argument('--unlearn-method', type=str, default=\"svd_unlearn\",\n",
        "                    help='')\n",
        "\n",
        "parser.add_argument('--salun-threshold', type=float, default=0.1,\n",
        "                    help='')\n",
        "\n",
        "parser.add_argument('--goel-exact', action=\"store_true\", default=False,\n",
        "                    help='')\n",
        "\n",
        "parser.add_argument('--ssd-lambda', type=float, default=1,\n",
        "                    help='')\n",
        "parser.add_argument('--ssd-alpha', type=float, default=10,\n",
        "                    help='')\n",
        "\n",
        "parser.add_argument('--scrub-del-bsz', type=int, default=512,\n",
        "                    help='')\n",
        "parser.add_argument('--scrub-sgda-bsz', type=int, default=64,\n",
        "                    help='')\n",
        "parser.add_argument('--scrub-msteps', type=int, default=2,\n",
        "                    help='')\n",
        "parser.add_argument('--scrub-epochs', type=int, default=3,\n",
        "                    help='')\n",
        "\n",
        "parser.add_argument('--our-alpha-r', type=int, default=100,\n",
        "                    help='')\n",
        "parser.add_argument('--our-alpha-f', type=int, default=3,\n",
        "                    help='')\n",
        "parser.add_argument('--our-samples', type=int, default=900,\n",
        "                    help='')\n",
        "parser.add_argument('--our-max-patches', type=int, default=10000,\n",
        "                    help='')\n",
        "\n",
        "\n",
        "parser.add_argument('--tarun-impair-lr', type=float, default=2e-4,\n",
        "                    help='')\n",
        "parser.add_argument('--tarun-samples-per-class', type=int, default=1000,\n",
        "                    help='')\n",
        "### wandb parameters\n",
        "parser.add_argument('--project-name', type=str, default='baseline',\n",
        "                    help='')\n",
        "parser.add_argument('--group-name', type=str, default='final',\n",
        "                    help='')\n",
        "parser.add_argument('--multiclass', action='store_true', default=False,\n",
        "                    help='For Saving the current Model')\n",
        "parser.add_argument('--class-names', type=str, default=None,\n",
        "                    help='')\n",
        "parser.add_argument('--do-mia',action='store_true', default=False,\n",
        "                    help='')\n",
        "parser.add_argument('--do-mia-ulira',action='store_true', default=False,\n",
        "                    help='')\n",
        "parser.add_argument('--plot-mia-roc',action='store_true', default=False,\n",
        "                    help='')\n",
        "\n",
        "args = argparse.Namespace()\n",
        "\n",
        "for action in parser._actions:\n",
        "  if hasattr(action, 'default'):\n",
        "    setattr(args, action.dest, action.default)\n",
        "\n",
        "args.train_transform = not args.no_train_transform\n",
        "if args.unlearn_class:\n",
        "    args.unlearn_class=[int(val) for val in args.unlearn_class.split(\",\")]\n",
        "else:\n",
        "    args.unlearn_class = []\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "train_kwargs = {'batch_size': args.batch_size}\n",
        "test_kwargs = {'batch_size': args.test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 2,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX3Fe7eWNngL"
      },
      "source": [
        "## Custom Parameters and Overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4WYWqrSANrsg"
      },
      "outputs": [],
      "source": [
        "#args.dataset = \"cifar10\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw3kAjv3MtVW"
      },
      "source": [
        "## Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwLkwzgzL-Lv",
        "outputId": "40a12664-0b38-4b98-ddaa-6e5ea9b453b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB-Training_fixed.zip to ../data/gtsrb/GTSRB-Training_fixed.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187M/187M [00:07<00:00, 23.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/gtsrb/GTSRB-Training_fixed.zip to ../data/gtsrb\n",
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip to ../data/gtsrb/GTSRB_Final_Test_Images.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 89.0M/89.0M [00:04<00:00, 20.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/gtsrb/GTSRB_Final_Test_Images.zip to ../data/gtsrb\n",
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip to ../data/gtsrb/GTSRB_Final_Test_GT.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99.6k/99.6k [00:00<00:00, 220kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/gtsrb/GTSRB_Final_Test_GT.zip to ../data/gtsrb\n",
            "<class 'int'>\n"
          ]
        }
      ],
      "source": [
        "dataset1, dataset2 = get_dataset(args)\n",
        "\n",
        "if args.dataset == \"svhn\":\n",
        "  dataset1.targets = [int(label) for label in dataset1.labels]\n",
        "elif args.dataset == \"gtsrb\":\n",
        "  dataset1.targets = [label for _, label in dataset1]\n",
        "\n",
        "\n",
        "print(type(dataset1.targets[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lcQ6GARqdra",
        "outputId": "f22ece19-505a-4c6b-9319-dfba23fc2e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label for the first image: 0\n"
          ]
        }
      ],
      "source": [
        "image, label = dataset1[0]\n",
        "print(f\"Label for the first image: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnS2mayIMu2j"
      },
      "source": [
        "## Get Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DiVMWL_TL_T5"
      },
      "outputs": [],
      "source": [
        "model = get_model(args, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mROoc0M_MwnO"
      },
      "source": [
        "## Prepare Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-pxwzTlIMAdI"
      },
      "outputs": [],
      "source": [
        "val_index= np.arange(len(dataset1))\n",
        "val_dataset = torch.utils.data.Subset(dataset1, val_index)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqFu2r4sMyUT"
      },
      "source": [
        "## Model Training optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oLz1Y--_MB3T"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=args.gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTbM2hwIM3Qq"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCTJf7AnMDY8",
        "outputId": "71e9ad80-aaf4-4730-a4b3-7d7952be800b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 1323.0939, Validation Loss: 2.0729\n",
            "Epoch 2/50, Train Loss: 536.4701, Validation Loss: 0.7347\n",
            "Epoch 3/50, Train Loss: 172.3136, Validation Loss: 0.6778\n",
            "Epoch 4/50, Train Loss: 71.3849, Validation Loss: 0.2999\n",
            "Epoch 5/50, Train Loss: 44.4299, Validation Loss: 0.2372\n",
            "Epoch 6/50, Train Loss: 32.4430, Validation Loss: 0.6988\n",
            "Epoch 7/50, Train Loss: 27.0330, Validation Loss: 0.1765\n",
            "Epoch 8/50, Train Loss: 23.4428, Validation Loss: 0.1924\n",
            "Epoch 9/50, Train Loss: 27.0736, Validation Loss: 0.1661\n",
            "Epoch 10/50, Train Loss: 23.3245, Validation Loss: 0.0946\n",
            "Epoch 11/50, Train Loss: 18.8125, Validation Loss: 0.0356\n",
            "Epoch 12/50, Train Loss: 18.8692, Validation Loss: 0.2035\n",
            "Epoch 13/50, Train Loss: 22.0943, Validation Loss: 0.3004\n",
            "Epoch 14/50, Train Loss: 19.2596, Validation Loss: 0.0596\n",
            "Epoch 15/50, Train Loss: 19.3486, Validation Loss: 0.0165\n",
            "Epoch 16/50, Train Loss: 16.3669, Validation Loss: 0.0427\n",
            "Epoch 17/50, Train Loss: 17.4579, Validation Loss: 0.1000\n",
            "Epoch 18/50, Train Loss: 19.7928, Validation Loss: 0.0434\n"
          ]
        }
      ],
      "source": [
        "# Optimizer and scheduler\n",
        "\n",
        "\n",
        "# Early stopping setup\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience = 10  # Stop after 10 epochs without improvement\n",
        "model.train()\n",
        "# Training loop\n",
        "args.epochs_or_steps = 50\n",
        "for epoch in range(1, args.epochs_or_steps + 1):\n",
        "    # Training step\n",
        "    train_loss = train(args, model, device, train_loader, optimizer, epoch, \"descent\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target)  # Use the same loss function as in training\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)  # Average loss over the validation set\n",
        "\n",
        "    print(f\"Epoch {epoch}/{args.epochs_or_steps}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "    torch.save(model.state_dict(), f\"resnet_34_gtsrb_tr{train_loss:.2f}_vl{val_loss:.2f}\")\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0  # Reset the patience counter\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break  # Stop training early\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Optionally, you can add a test evaluation at the end of training\n",
        "    if epoch == args.epochs_or_steps:\n",
        "        # Test the model on the test set if needed\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                loss = F.nll_loss(output, target)  # Define your loss function\n",
        "                test_loss += loss.item()\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTSHsu-NM6i8"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfhmYDeSMHoN"
      },
      "outputs": [],
      "source": [
        "# Load the state_dict from the .pth file\n",
        "'''\n",
        "checkpoint_path = \"/content/best_model_resnet34.pth\"  # Replace with your path\n",
        "state_dict = torch.load(checkpoint_path)\n",
        "model = get_model(args, device)\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(state_dict)\n",
        "'''\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)  # Your loss function\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = output.max(1)  # Get index of max log-probability\n",
        "        correct += predicted.eq(target).sum().item()  # Count correct predictions\n",
        "        total += target.size(0)  # Total samples\n",
        "\n",
        "# Average test loss\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "accuracy = 100.0 * correct / total\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyM51m3A_b-V"
      },
      "outputs": [],
      "source": [
        "# Load the state_dict from the .pth file\n",
        "'''\n",
        "checkpoint_path = \"/content/best_model_resnet34.pth\"  # Replace with your path\n",
        "state_dict = torch.load(checkpoint_path)\n",
        "model = get_model(args, device)\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(state_dict)\n",
        "'''\n",
        "model.eval()\n",
        "train_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)  # Your loss function\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = output.max(1)  # Get index of max log-probability\n",
        "        correct += predicted.eq(target).sum().item()  # Count correct predictions\n",
        "        total += target.size(0)  # Total samples\n",
        "\n",
        "# Average test loss\n",
        "train_loss /= len(train_loader)\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "accuracy = 100.0 * correct / total\n",
        "\n",
        "print(f\"train Loss: {test_loss:.4f}\")\n",
        "print(f\"train Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnzem7q3_zEL"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"gtsrb_resnet34.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyQDOBpWND8G"
      },
      "source": [
        "## Load Pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PIgrZK3MJot"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/gtsrb_resnet34.pth\"  # Replace with your path\n",
        "state_dict = torch.load(checkpoint_path)\n",
        "args.num_classes = 43\n",
        "model = get_model(args, device)\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzJ0-QzjMO2k"
      },
      "outputs": [],
      "source": [
        "retain_dataset, forget_dataset = get_retain_forget_partition(args, dataset1, args.unlearn_class)\n",
        "retain_loader = torch.utils.data.DataLoader(retain_dataset,**train_kwargs)\n",
        "forget_loader = torch.utils.data.DataLoader(forget_dataset,**train_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqBkbyrQga5Q"
      },
      "outputs": [],
      "source": [
        "retain_dataset, forget_dataset = get_retain_forget_partition(args, dataset1, args.unlearn_class)\n",
        "print(len(retain_dataset), len(forget_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdgZPFHCNGXT"
      },
      "source": [
        "## Unlearn Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7au0_NsfiO4L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def display_images_with_labels(data_loader, num_images=16, mean=(0.4377, 0.4438, 0.4728), std=(0.1980, 0.2010, 0.1970)):\n",
        "    \"\"\"\n",
        "    Display individual images along with their labels in a grid.\n",
        "\n",
        "    Args:\n",
        "        data_loader (DataLoader): PyTorch DataLoader containing the dataset.\n",
        "        num_images (int, optional): Number of images to display. Default is 16.\n",
        "        mean (tuple, optional): Mean values for each channel used in normalization.\n",
        "        std (tuple, optional): Standard deviation for each channel used in normalization.\n",
        "    \"\"\"\n",
        "    # Fetch a single batch of images and labels\n",
        "    images, labels = next(iter(data_loader))\n",
        "\n",
        "    # Select only the required number of images and labels\n",
        "    images = images[:num_images]\n",
        "    labels = labels[:num_images]\n",
        "\n",
        "    # Unnormalize the images\n",
        "    images = images.clone()\n",
        "    for i in range(images.size(0)):\n",
        "        for c in range(3):  # Assuming 3 RGB channels\n",
        "            images[i, c] = images[i, c] * std[c] + mean[c]\n",
        "\n",
        "    # Clamp the values between 0 and 1\n",
        "    images = torch.clamp(images, 0, 1)\n",
        "\n",
        "    # Define the grid size\n",
        "    grid_size = int(np.ceil(np.sqrt(num_images)))\n",
        "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size * 3, grid_size * 3))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Plot each image with its corresponding label\n",
        "    for i, (image, label) in enumerate(zip(images, labels)):\n",
        "        img_np = image.permute(1, 2, 0).numpy()  # Convert to NumPy format\n",
        "        axes[i].imshow(img_np)\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f\"Label: {label.item()}\", fontsize=10, pad=10)\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(len(images), len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    # Adjust layout and show\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# display_images_with_labels(data_loader, num_images=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13GFm80diby5"
      },
      "outputs": [],
      "source": [
        "visualize_samples(retain_loader, num_images=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXPLM1gjVhN_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SubSet(Dataset):\n",
        "    r\"\"\"\n",
        "    Subset of a dataset at specified indices.\n",
        "\n",
        "    Arguments:\n",
        "        dataset (Dataset): The whole Dataset.\n",
        "        indices (sequence): Indices in the whole set selected for subset.\n",
        "        labels (sequence): Targets as required for the indices. Will be the same length as indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, indices):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        labels = torch.LongTensor([self.dataset.targets[i] for i in indices])\n",
        "        labels_hold = torch.ones(len(dataset)).type(torch.long) * 10000  # Placeholder for invalid indices\n",
        "        labels_hold[self.indices] = labels\n",
        "        self.labels = labels_hold\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.dataset[self.indices[idx]][0]\n",
        "        label = self.labels[self.indices[idx]]\n",
        "        return (image, label)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of samples in this subset\n",
        "        return len(self.indices)\n",
        "\n",
        "    def update_labels(self, new_labels):\n",
        "        labels_hold = torch.ones(len(self.dataset)).type(torch.long) * 10000  # Placeholder for invalid indices\n",
        "        labels_hold[self.indices] = torch.LongTensor(new_labels)\n",
        "        self.labels = labels_hold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42BT6HSVVYtx"
      },
      "outputs": [],
      "source": [
        "def get_retain_forget_partition(args, dataset, unlearn_class_list, return_ind = False):\n",
        "    retain_ind = []\n",
        "    forget_ind = []\n",
        "    for sample_index in range(len(dataset)) :\n",
        "        if (torch.is_tensor(dataset.targets[sample_index])):\n",
        "            sample_class = int(dataset.targets[sample_index].item())\n",
        "        elif isinstance(dataset.targets[sample_index], int ):\n",
        "            sample_class = dataset.targets[sample_index]\n",
        "        elif isinstance(dataset.targets[sample_index], np.integer):\n",
        "          sample_class = int(dataset.targets[sample_index])\n",
        "        if sample_class in unlearn_class_list:\n",
        "            forget_ind.append(sample_index)\n",
        "        else:\n",
        "            retain_ind.append(sample_index)\n",
        "    retain_dataset = SubSet(dataset, retain_ind)\n",
        "    forget_dataset = SubSet(dataset, forget_ind)\n",
        "    if return_ind:\n",
        "        return retain_dataset, forget_dataset, retain_ind, forget_ind\n",
        "\n",
        "    return retain_dataset, forget_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCMu5bA-MQGR"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "max_iterations = 100\n",
        "\n",
        "for unlearn_class in range(args.num_classes):\n",
        "    args.unlearn_class = [unlearn_class]\n",
        "    print(f\"unlearning class {unlearn_class}\")\n",
        "    unlearn_model = copy.deepcopy(model)\n",
        "    retain_dataset, forget_dataset = get_retain_forget_partition(args, dataset1, args.unlearn_class)\n",
        "    retain_loader = torch.utils.data.DataLoader(retain_dataset,**train_kwargs)\n",
        "    forget_loader = torch.utils.data.DataLoader(forget_dataset,**train_kwargs)\n",
        "    unlearn_model = svd_unlearn(\n",
        "            args = args,\n",
        "            model = unlearn_model,\n",
        "            device = device,\n",
        "            retain_loader= retain_loader,\n",
        "            forget_loader = forget_loader,\n",
        "            train_loader = val_loader if args.val_set_mode else train_loader,\n",
        "            test_loader= test_loader,\n",
        "            optimizer = optimizer,\n",
        "            epochs = args.epochs_or_steps,\n",
        "            max_steps = args.epochs_or_steps,\n",
        "            train_dataset = dataset1,\n",
        "            val_index =val_index\n",
        "    )\n",
        "    train_retain_acc, train_forget_acc, train_metric = test(unlearn_model, device, train_loader, torch.tensor(args.unlearn_class).to(device), args.class_label_names, args.num_classes,\n",
        "    job_name = args.unlearn_method, set_name=\"Final Train Set\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gumk2hdQNJx6"
      },
      "source": [
        "## Test Unlearned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bllngNTxMR0s"
      },
      "outputs": [],
      "source": [
        "train_retain_acc, train_forget_acc, train_metric = test(model, device, train_loader, torch.tensor(args.unlearn_class).to(device), args.class_label_names, args.num_classes,\n",
        "    job_name = args.unlearn_method, set_name=\"Final Train Set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tnDRvoI8uwP"
      },
      "source": [
        "## Load Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8czrW3uc8h29"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/pretrained_models/cifar100_vgg11_bn.pt\"  # Replace with your path\n",
        "state_dict = torch.load(checkpoint_path)\n",
        "args.num_classes = 100\n",
        "model = get_model(args, device)\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJGTqrdn87BJ"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib7nnj138_iQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_images_from_loader(loader, num_images_per_class, num_classes):\n",
        "  \"\"\"\n",
        "  Fetches a specified number of images per class from a DataLoader.\n",
        "\n",
        "  Args:\n",
        "    loader: The DataLoader to fetch images from.\n",
        "    num_images_per_class: The number of images to fetch for each class.\n",
        "    num_classes: The total number of classes in the dataset.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary where keys are class labels and values are lists of images\n",
        "    in numpy format.\n",
        "  \"\"\"\n",
        "\n",
        "  images_by_class = {}\n",
        "  counts_by_class = [0] * num_classes\n",
        "\n",
        "  for images, labels in loader:\n",
        "    for i, label in enumerate(labels):\n",
        "      label_idx = label.item()\n",
        "      if counts_by_class[label_idx] < num_images_per_class:\n",
        "        if label_idx not in images_by_class:\n",
        "          images_by_class[label_idx] = []\n",
        "        # Convert image to numpy and add to the list\n",
        "        image_np = images[i].numpy()\n",
        "        images_by_class[label_idx].append(image_np)\n",
        "        counts_by_class[label_idx] += 1\n",
        "\n",
        "      if all(count >= num_images_per_class for count in counts_by_class):\n",
        "        return images_by_class\n",
        "\n",
        "  # If not enough images were found, return what was collected\n",
        "  return images_by_class\n",
        "\n",
        "'''\n",
        "# Get 10 images from each class\n",
        "images_by_class = get_images_from_loader(train_loader, 10, args.num_classes)\n",
        "\n",
        "# Print the shape of the first image from the first class (for verification)\n",
        "for class_idx, images in images_by_class.items():\n",
        "  print(f\"Class {class_idx}: {len(images)} images\")\n",
        "  if images:\n",
        "    print(f\"  Example image shape: {images[0].shape}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlsTLdaI_wub"
      },
      "source": [
        "# Zero Shot Gradient Free Unlearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1iECGbGAfFk"
      },
      "outputs": [],
      "source": [
        "val_index= np.arange(len(dataset1))\n",
        "val_dataset = torch.utils.data.Subset(dataset1, val_index)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGLMiICYArzb"
      },
      "outputs": [],
      "source": [
        "#retain_dataset, forget_dataset = get_retain_forget_partition(args, dataset1, args.unlearn_class)\n",
        "#retain_loader = torch.utils.data.DataLoader(retain_dataset,**train_kwargs)\n",
        "#forget_loader = torch.utils.data.DataLoader(forget_dataset,**train_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PegigBqsBO7u"
      },
      "outputs": [],
      "source": [
        "img_shape = train_loader.dataset[0][0].shape\n",
        "target_vector = [1 if i == args.unlearn_class else 0 for i in range(args.num_classes)]\n",
        "target_class = args.unlearn_class\n",
        "args.img_shape = train_loader.dataset[0][0].shape\n",
        "args.target_samples = 900"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hj-Xe46B474"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def synthesize_images_for_class(model, device, target_class, num_samples, img_shape,\n",
        "                                lr=0.1, steps=800, regularization_weight=1e-4,\n",
        "                                debug=False):\n",
        "    \"\"\"\n",
        "    Synthesize images that the model classifies as `target_class` starting from noise.\n",
        "    Stopping criteria:\n",
        "      If an image's predicted class equals target_class, we consider it done.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize random noise in [0,1]\n",
        "    images = torch.randn(num_samples, *img_shape, device=device, requires_grad=True)\n",
        "    images.data = (images.data - images.data.min()) / (images.data.max() - images.data.min() + 1e-5)\n",
        "\n",
        "    optimizer = torch.optim.Adam([images], lr=lr)\n",
        "\n",
        "    done_mask = torch.zeros(num_samples, dtype=torch.bool, device=device)\n",
        "\n",
        "    for step in range(steps):\n",
        "        if done_mask.all():\n",
        "            if debug:\n",
        "                print(f\"All samples reached target class by step {step}. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        class_scores = outputs[:, target_class]\n",
        "        loss = -class_scores.mean() + regularization_weight * (images**2).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clamp images to [0,1]\n",
        "        images.data.clamp_(0, 1)\n",
        "\n",
        "        # Check predictions\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        previously_done_count = done_mask.sum().item()\n",
        "        done_mask = done_mask | (preds == target_class)\n",
        "        currently_done_count = done_mask.sum().item()\n",
        "\n",
        "        if debug:\n",
        "            newly_done = currently_done_count - previously_done_count\n",
        "            print(f\"Step {step+1}/{steps}: {currently_done_count}/{num_samples} images done (+{newly_done} this step).\")\n",
        "\n",
        "    if debug and not done_mask.all():\n",
        "        print(f\"Reached max steps without all images classified as target class. \"\n",
        "              f\"{done_mask.sum().item()} out of {num_samples} done.\")\n",
        "\n",
        "    return images.detach()\n",
        "\n",
        "\n",
        "def get_class_samples_from_noise(model, device, args, retain_extra, forget_extra):\n",
        "    \"\"\"\n",
        "    Generate synthetic datasets (forget_loader and retain_loader) from noise.\n",
        "    - forget_loader: corresponds to args.unlearn_class\n",
        "    - retain_loader: corresponds to the other classes\n",
        "\n",
        "    Assumptions:\n",
        "      - CIFAR-10: num_classes=10\n",
        "      - target_samples = args.target_samples\n",
        "      - For other classes: target_samples total, distributed equally among them.\n",
        "      - img_shape = (C, H, W), e.g. (3,32,32) for CIFAR-10.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure unlearn_class is an integer\n",
        "    unlearn_class = args.unlearn_class\n",
        "    if isinstance(unlearn_class, (list, tuple)):\n",
        "        # If it's a list or tuple, extract the first element (assuming single value)\n",
        "        unlearn_class = unlearn_class[0]\n",
        "    if isinstance(unlearn_class, torch.Tensor):\n",
        "        # If it's a tensor with one element, convert to int\n",
        "        unlearn_class = unlearn_class.item()\n",
        "    unlearn_class = int(unlearn_class)\n",
        "\n",
        "    img_shape = args.img_shape\n",
        "    target_samples = args.target_samples\n",
        "    num_classes = args.num_classes  # e.g., 10 for CIFAR-10\n",
        "\n",
        "    # Synthesize images for the unlearn_class\n",
        "    forget_images = synthesize_images_for_class(\n",
        "        model=model,\n",
        "        device=device,\n",
        "        target_class=unlearn_class,\n",
        "        num_samples=target_samples,\n",
        "        img_shape=img_shape,\n",
        "        lr=0.1,\n",
        "        steps=300,\n",
        "        regularization_weight=1e-4,\n",
        "        debug=False\n",
        "    )\n",
        "\n",
        "    # For other classes, generate target_samples total, split evenly\n",
        "    other_class_samples = target_samples\n",
        "    samples_per_other_class = other_class_samples // (num_classes - 1)+retain_extra\n",
        "    #samples_per_other_class = target_samples\n",
        "    retain_images_list = []\n",
        "    retain_labels_list = []\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        if c == unlearn_class:\n",
        "            continue\n",
        "        class_images = synthesize_images_for_class(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            target_class=c,\n",
        "            num_samples=samples_per_other_class,\n",
        "            img_shape=img_shape,\n",
        "            lr=0.1,\n",
        "            steps=300,\n",
        "            regularization_weight=1e-4,\n",
        "            debug=False\n",
        "        )\n",
        "        retain_images_list.append(class_images)\n",
        "        retain_labels_list.extend([c] * samples_per_other_class)\n",
        "\n",
        "    retain_images = torch.cat(retain_images_list, dim=0)\n",
        "\n",
        "    # Create labels for forget images\n",
        "    forget_labels = torch.full((forget_images.size(0),), unlearn_class, dtype=torch.long)\n",
        "    retain_labels = torch.tensor(retain_labels_list, dtype=torch.long)\n",
        "\n",
        "    # Move labels to CPU if needed, as DataLoader works well with CPU tensors by default\n",
        "    # If you prefer on GPU, you can leave them on device, but typically datasets are on CPU.\n",
        "    forget_labels = forget_labels.cpu()\n",
        "    retain_labels = retain_labels.cpu()\n",
        "\n",
        "    forget_images = forget_images.cpu()\n",
        "    retain_images = retain_images.cpu()\n",
        "\n",
        "    forget_dataset = TensorDataset(forget_images, forget_labels)\n",
        "    retain_dataset = TensorDataset(retain_images, retain_labels)\n",
        "\n",
        "    forget_loader = DataLoader(forget_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    retain_loader = DataLoader(retain_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    return forget_loader, retain_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyK7zbAz-YU3"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "max_iterations = 100\n",
        "args.target_samples = 42*50\n",
        "\n",
        "for unlearn_class in range(args.num_classes):\n",
        "    args.unlearn_class = [unlearn_class]\n",
        "    print(f\"unlearning class {unlearn_class}\")\n",
        "    unlearn_model = copy.deepcopy(model)\n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"iteration {iteration}\")\n",
        "        forget_loader, retain_loader = get_class_samples_from_noise(model, device, args, retain_extra = 200*iteration, forget_extra = 5*iteration)\n",
        "        unlearn_model = svd_unlearn(\n",
        "                args = args,\n",
        "                model = unlearn_model,\n",
        "                device = device,\n",
        "                retain_loader= retain_loader,\n",
        "                forget_loader = forget_loader,\n",
        "                train_loader = val_loader if args.val_set_mode else train_loader,\n",
        "                test_loader= test_loader,\n",
        "                optimizer = optimizer,\n",
        "                epochs = args.epochs_or_steps,\n",
        "                max_steps = args.epochs_or_steps,\n",
        "                train_dataset = dataset1,\n",
        "                val_index =val_index,\n",
        "                custom=True\n",
        "        )\n",
        "        train_retain_acc, train_forget_acc, train_metric = test(unlearn_model, device, train_loader, torch.tensor(args.unlearn_class).to(device), args.class_label_names, args.num_classes,\n",
        "        job_name = args.unlearn_method, set_name=\"Final Train Set\")\n",
        "        print(\"train forget acc: \", train_forget_acc)\n",
        "        if train_forget_acc < 0.1:\n",
        "          break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frvrYR26ZHef"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QF1AkRw9vU8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assume forget_loader and retain_loader are already created.\n",
        "\n",
        "# Get one batch from the forget_loader\n",
        "forget_batch = next(iter(forget_loader))\n",
        "forget_images, forget_labels = forget_batch\n",
        "\n",
        "# Get one batch from the retain_loader\n",
        "retain_batch = next(iter(retain_loader))\n",
        "retain_images, retain_labels = retain_batch\n",
        "\n",
        "def show_images(images, labels, title, num_images=8):\n",
        "    # Convert to CPU and detach if needed\n",
        "    imgs = images[:num_images].cpu().detach()\n",
        "    labs = labels[:num_images].cpu().detach()\n",
        "\n",
        "    # For CIFAR-10 images are typically [C,H,W], with C=3\n",
        "    # If you used the code above, images should already be in [0,1] range.\n",
        "    plt.figure(figsize=(12, 2))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        img = imgs[i].permute(1, 2, 0).numpy()  # Convert to HWC for matplotlib\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Class: {labs[i].item()}\")\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "# Show a few forget images\n",
        "show_images(forget_images, forget_labels, title=\"Forget Images\")\n",
        "\n",
        "# Show a few retain images\n",
        "show_images(retain_images, retain_labels, title=\"Retain Images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAskz372gQjf"
      },
      "outputs": [],
      "source": [
        "unlearn_model = svd_unlearn(\n",
        "    args = args,\n",
        "    model = unlearn_model,\n",
        "    device = device,\n",
        "    retain_loader= retain_loader,\n",
        "    forget_loader = forget_loader,\n",
        "    train_loader = val_loader if args.val_set_mode else train_loader,\n",
        "    test_loader= test_loader,\n",
        "    optimizer = optimizer,\n",
        "    epochs = args.epochs_or_steps,\n",
        "    max_steps = args.epochs_or_steps,\n",
        "    train_dataset = dataset1,\n",
        "    val_index =val_index,\n",
        "    custom=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty4C8hGHgPuV"
      },
      "outputs": [],
      "source": [
        "print(len(forget_loader))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}